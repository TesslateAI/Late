# Llama 3.2 3B Instruct - LoRA Fine-Tuning
# Optimized for consumer GPUs (16-24GB VRAM)

# Model and Dataset
base_model: "unsloth/Llama-3.2-3B-Instruct"
dataset_name: "mlabonne/FineTome-100k"  # High-quality curated dataset
output_model_name: "your-username/llama3.2-3b-lora-custom"
output_dir: "/workspace/outputs/llama3.2-lora/"

# Training Configuration
training_type: "lora"
max_seq_length: 2048  # Reasonable for 3B model

# Unsloth Integration (optional) - Provides significant speedup on both AMD and NVIDIA GPUs
use_unsloth: true

# Loss Masking Strategy (optional, defaults to "full")
# Options: "full" (compute loss on full conversation) or "assistant_only" (mask user prompts)
loss_masking_strategy: "full"  # Default, can be omitted

# Hyperparameters for smaller model
batch_size: 4  # Works on 16GB GPUs
gradient_accumulation: 4  # Effective batch size: 16
epochs: 3
learning_rate: 0.0002
lr_scheduler_type: "cosine"
optim: "adamw_torch_fused"
save_steps: 100

# Memory Optimization
gradient_checkpointing: true
torch_compile: true
tf32: false  # Disabled for AMD GPUs
cache_dir: "~/.cache/late/models"

# LoRA Configuration for 3B model
lora:
  r: 64  # Good balance for 3B model
  lora_alpha: 128
  lora_dropout: 0.05
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

# Logging
report_to_wandb: false  # Disabled for now
upload_to_hub: false  # Disabled for now

# Optional: Notifications via ntfy.sh (leave empty to disable)
ntfy_topic: ""  # e.g., "my-training-runs"

# Optional: HuggingFace Token (uses HF_TOKEN env var if not set)
hf_token: ""  # or "${HF_TOKEN}"
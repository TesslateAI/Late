# Llama 3 Comprehensive LoRA Hyperparameter Sweep
# Optimize LoRA configuration for best performance vs efficiency

sweep_id: "llama3_lora_optimization"

# Sweep LoRA-specific parameters
sweep_parameters:
  learning_rate: [1e-4, 3e-4, 5e-4]
  lora.r: [32, 64, 128, 256]  # Trade-off between performance and parameters
  lora.lora_alpha: [64, 128, 256, 512]  # Scaling factor
  lora.lora_dropout: [0.05, 0.1, 0.2]

# Fixed parameters for consistent comparison
overrides:
  max_seq_length: 2048
  batch_size: 2
  gradient_accumulation: 8
  epochs: 1
  gradient_checkpointing: true
  save_steps: 50
  
# Run for fixed steps to ensure fair comparison
early_stop:
  max_steps: 200
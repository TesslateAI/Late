# Full Loss Masking Strategy Example
# This is the DEFAULT strategy - computes loss on the entire conversation

# Model and Dataset
base_model: "meta-llama/Llama-3.2-3B-Instruct"
dataset_name: "yahma/alpaca-cleaned"
output_model_name: "your-username/llama3-full-masking"
output_dir: "./outputs/full-masking/"

# Training Configuration
training_type: "sft"  # or "lora"
max_seq_length: 2048

# ⭐ Loss Masking Strategy (DEFAULT: "full")
# Computes loss on entire conversation (simpler, faster preprocessing)
loss_masking_strategy: "full"  # This is the default, can be omitted

# Hyperparameters
batch_size: 4
gradient_accumulation: 4  # Effective batch size: 16
epochs: 1
learning_rate: 2e-5
lr_scheduler_type: "cosine"
optim: "adamw_torch_fused"
save_steps: 100

# Memory Optimization
gradient_checkpointing: true
torch_compile: true
tf32: true
cache_dir: "~/.cache/late/models"

# Logging and Upload
report_to_wandb: false
upload_to_hub: false

# Optional: Notifications (leave empty to disable)
ntfy_topic: ""

# Optional: HuggingFace Token (uses HF_TOKEN env var if not set)
hf_token: ""

# When to use Full Loss Masking:
# ✅ Simpler and faster preprocessing
# ✅ Learn from complete conversations
# ✅ Good for most use cases
# ✅ Recommended for beginners
